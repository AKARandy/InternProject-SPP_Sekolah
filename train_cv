{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13181105,"sourceType":"datasetVersion","datasetId":8353043},{"sourceId":13298188,"sourceType":"datasetVersion","datasetId":8428640}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Load & Clean Data ===\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, GridSearchCV, cross_val_predict\nfrom sklearn.metrics import mean_absolute_error, r2_score, make_scorer\nfrom catboost import CatBoostRegressor\nimport warnings\nfrom tqdm import tqdm\nimport time\nimport joblib\nfrom datetime import datetime\nimport json\nimport os\nwarnings.filterwarnings('ignore')\n\n# Load data\ndf = pd.read_csv('/kaggle/input/ashw-tdi-salah/Gabungan_Surabaya_Sidoarjo.csv')\nprint(f\"Original dataset size: {len(df)}\")\n\n# Clean SPP column\ndf['SPP'] = pd.to_numeric(df['SPP'], errors='coerce').fillna(0).astype(int)\ndf = df[df['SPP'] > 0]\nprint(f\"After removing SPP=0: {len(df)}\")\n\n# OPTION: Uncomment these lines to apply additional filters\n#df = df[df['Peserta Didik'] >= 100]\n#print(f\"After student filter (>=100): {len(df)}\")\n# df = df[df['SPP'] <= 3300000]\n# print(f\"After SPP cap (<=3.3M): {len(df)}\")\n\n# Change Kurikulum for specific Yayasan\n#df.loc[df['Yayasan'] == 'YAYASAN PEMBINA UNIVERSITAS NEGERI JAKARTA', 'Kurikulum'] = 'Kurikulum Internasional'\n\n# Drop columns\ncolumns_to_drop = ['Yayasan', 'Website', 'NPSN']\ndf = df.drop(columns=columns_to_drop, errors='ignore')\n\n# Fill missing values with mean\ncolumns_to_fill = ['meter listrik', 'harga_listrik_per_bulan', 'estimasi harga tanah sekolah', \n                   'Tanggal SK Pendirian', 'Umur', 'luas tanah']\nfor col in columns_to_fill:\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n    mean_value = df[df[col] != 0][col].mean()\n    df[col] = df[col].replace(0, mean_value)\n    df[col] = df[col].fillna(mean_value)\n\n# Feature engineering\ndf['rombel_luas_ratio'] = df['Rombel'] / df['luas tanah']\ndf['total_rooms'] = df['R.Kelas'] + df['R.Lab'] + df['R.Perpus']\ndf['total_rooms_luas_ratio'] = df['total_rooms'] / df['luas tanah']\ndf['total_rooms_listrik_ratio'] = df['total_rooms'] / df['meter listrik']\ndf['listrik_cost_rooms_ratio'] = df['harga_listrik_per_bulan'] / df['total_rooms']\n\n# Replace infinite values\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\nfor col in df.select_dtypes(include=[np.float64, np.int64]).columns:\n    median_val = df[col].median()\n    df[col].fillna(median_val, inplace=True)\n\n# Create target variables\ndf['SPP_log'] = np.log1p(df['SPP'])\n\nprint(f\"\\nFinal dataset size for training: {len(df)}\")\n\n# === Data Distribution Analysis ===\nprint(\"\\n\" + \"=\"*50)\nprint(\"DATA DISTRIBUTION ANALYSIS\")\nprint(\"=\"*50)\nprint(f\"SPP Statistics:\")\nprint(f\"  Min: {df['SPP'].min():,.0f}\")\nprint(f\"  Max: {df['SPP'].max():,.0f}\")\nprint(f\"  Mean: {df['SPP'].mean():,.0f}\")\nprint(f\"  Median: {df['SPP'].median():,.0f}\")\nprint(f\"\\nSPP Distribution:\")\nprint(f\"  < 1M: {len(df[df['SPP'] < 1_000_000])} ({len(df[df['SPP'] < 1_000_000])/len(df)*100:.1f}%)\")\nprint(f\"  1M-3M: {len(df[(df['SPP'] >= 1_000_000) & (df['SPP'] < 3_000_000)])} ({len(df[(df['SPP'] >= 1_000_000) & (df['SPP'] < 3_000_000)])/len(df)*100:.1f}%)\")\nprint(f\"  >= 3M: {len(df[df['SPP'] >= 3_000_000])} ({len(df[df['SPP'] >= 3_000_000])/len(df)*100:.1f}%)\")\n\nprint(f\"\\nStudent Count (Peserta Didik):\")\nprint(f\"  Min: {df['Peserta Didik'].min():.0f}\")\nprint(f\"  Max: {df['Peserta Didik'].max():.0f}\")\nprint(f\"  Mean: {df['Peserta Didik'].mean():.0f}\")\nprint(f\"  < 100 students: {len(df[df['Peserta Didik'] < 100])} schools\")\n\n# === Prepare Features ===\ntarget = 'SPP_log'\nexclude_cols = ['SPP', 'SPP_log', 'Nama_sekolah']\nfeatures = [col for col in df.columns if col not in exclude_cols]\n\n# Identify categorical features\ncategorical_features = []\nfor col in features:\n    if col != 'R.Perpus' and (df[col].dtype == 'object' or df[col].nunique() < 10):\n        categorical_features.append(col)\n\nprint(f\"\\nCategorical features: {categorical_features}\")\nprint(f\"Total features: {len(features)}\")\n\n# Prepare X and y (FULL DATASET)\nX = df[features].copy()\ny = df[target].copy()\ny_spp_original = df['SPP'].copy()  # Keep original SPP for range analysis\n\nprint(f\"\\nFull dataset for modeling: {X.shape}\")\n\n# Convert categorical features to string\nfor col in categorical_features:\n    X[col] = X[col].astype(str)\n\n# === Helper Functions ===\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)[y_true != 0]) * 100\n\ndef calculate_spp_range_metrics(y_true_spp, y_pred_spp):\n    \"\"\"Calculate metrics for different SPP ranges\"\"\"\n    ranges = {\n        'SPP_<1M': (y_true_spp < 1_000_000),\n        'SPP_1M-3M': (y_true_spp >= 1_000_000) & (y_true_spp < 3_000_000),\n        'SPP_>=3M': (y_true_spp >= 3_000_000)\n    }\n    \n    metrics = {}\n    for range_name, mask in ranges.items():\n        if mask.sum() > 0:\n            y_true_range = y_true_spp[mask]\n            y_pred_range = y_pred_spp[mask]\n            metrics[f'{range_name}_MAE'] = mean_absolute_error(y_true_range, y_pred_range)\n            metrics[f'{range_name}_MAPE'] = mean_absolute_percentage_error(y_true_range, y_pred_range)\n            metrics[f'{range_name}_count'] = int(mask.sum())\n        else:\n            metrics[f'{range_name}_MAE'] = np.nan\n            metrics[f'{range_name}_MAPE'] = np.nan\n            metrics[f'{range_name}_count'] = 0\n    \n    return metrics\n\n# === Hyperparameter Grid ===\nparam_grid = {\n    'iterations': [600],\n    'learning_rate': [0.05, 0.1, 0.15],\n    'depth': [4, 6, 8],\n    'l2_leaf_reg': [1, 3, 5],\n    'border_count': [32, 64, 128],\n    'random_strength': [0.5, 1.0, 1.5]\n}\n\nprint(f\"\\nHyperparameter combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n\n# === K-Fold Cross-Validation Setup ===\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# === Base Model ===\nbase_model = CatBoostRegressor(\n    random_seed=42,\n    cat_features=categorical_features,\n    verbose=False,\n    early_stopping_rounds=50,\n    task_type='GPU'\n)\n\n# === Custom Scorers ===\ndef mape_scorer(y_true, y_pred):\n    y_true_orig = np.expm1(y_true)\n    y_pred_orig = np.expm1(y_pred)\n    return -mean_absolute_percentage_error(y_true_orig, y_pred_orig)\n\ndef mae_original_scorer(y_true, y_pred):\n    y_true_orig = np.expm1(y_true)\n    y_pred_orig = np.expm1(y_pred)\n    return -mean_absolute_error(y_true_orig, y_pred_orig)\n\ndef r2_original_scorer(y_true, y_pred):\n    return r2_score(np.expm1(y_true), np.expm1(y_pred))\n\nscoring = {\n    'neg_mae': make_scorer(mae_original_scorer),\n    'neg_mape': make_scorer(mape_scorer),\n    'r2_log': 'r2',\n    'r2_original': make_scorer(r2_original_scorer)\n}\n\n# === GridSearchCV on FULL Dataset ===\nprint(\"\\n\" + \"=\"*50)\nprint(\"STARTING GRID SEARCH WITH K-FOLD CV ON FULL DATA\")\nprint(\"=\"*50)\nprint(f\"This will fit {np.prod([len(v) for v in param_grid.values()]) * cv.n_splits} models\")\nprint(f\"Using all {len(X)} samples for cross-validation\")\nprint(\"Estimated time: 2-4 hours (depending on GPU)\")\nprint(\"=\"*50 + \"\\n\")\n\nstart_time = time.time()\n\ngrid_search = GridSearchCV(\n    base_model,\n    param_grid,\n    cv=cv,\n    scoring=scoring,\n    refit='r2_original',\n    verbose=2,\n    n_jobs=1,\n    return_train_score=True\n)\n\ngrid_search.fit(X, y)\n\nend_time = time.time()\nprint(f\"\\n\\nGrid Search completed in {(end_time - start_time)/60:.2f} minutes\")\n\n# === Extract All Metrics from GridSearch Results ===\nprint(\"\\n\" + \"=\"*50)\nprint(\"EXTRACTING METRICS FROM GRID SEARCH RESULTS\")\nprint(\"=\"*50)\n\ncv_results = pd.DataFrame(grid_search.cv_results_)\n\nall_results = []\nfor idx, row in cv_results.iterrows():\n    params = row['params']\n    \n    result = {\n        'params': str(params),\n        'iterations': params['iterations'],\n        'learning_rate': params['learning_rate'],\n        'depth': params['depth'],\n        'l2_leaf_reg': params['l2_leaf_reg'],\n        'border_count': params['border_count'],\n        'random_strength': params['random_strength'],\n        'MAE_mean': -row['mean_test_neg_mae'],\n        'MAE_std': row['std_test_neg_mae'],\n        'MAPE_mean': -row['mean_test_neg_mape'],\n        'MAPE_std': row['std_test_neg_mape'],\n        'R2_log_mean': row['mean_test_r2_log'],\n        'R2_log_std': row['std_test_r2_log'],\n        'R2_original_mean': row['mean_test_r2_original'],\n        'R2_original_std': row['std_test_r2_original']\n    }\n    all_results.append(result)\n\nresults_df = pd.DataFrame(all_results)\nprint(f\"✓ Extracted metrics for {len(results_df)} parameter combinations\")\n\n# === Best Parameters ===\nprint(\"\\n\" + \"=\"*50)\nprint(\"BEST PARAMETERS FROM CV (BY R² ORIGINAL)\")\nprint(\"=\"*50)\nprint(grid_search.best_params_)\nprint(f\"\\nBest R² (original): {grid_search.best_score_:.4f}\")\n\n# === Save Rankings by Different Metrics ===\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# 1. Sorted by R² original\nresults_by_r2_orig = results_df.sort_values('R2_original_mean', ascending=False).reset_index(drop=True)\nresults_by_r2_orig.insert(0, 'Rank', range(1, len(results_by_r2_orig) + 1))\nr2_orig_filename = f'all_params_ranked_by_R2_original_{timestamp}.csv'\nresults_by_r2_orig.to_csv(r2_orig_filename, index=False)\nprint(f\"\\n✓ R² (original) rankings saved: {r2_orig_filename}\")\n\n# 2. Sorted by R² log\nresults_by_r2_log = results_df.sort_values('R2_log_mean', ascending=False).reset_index(drop=True)\nresults_by_r2_log.insert(0, 'Rank', range(1, len(results_by_r2_log) + 1))\nr2_log_filename = f'all_params_ranked_by_R2_log_{timestamp}.csv'\nresults_by_r2_log.to_csv(r2_log_filename, index=False)\nprint(f\"✓ R² (log) rankings saved: {r2_log_filename}\")\n\n# 3. Sorted by MAE\nresults_by_mae = results_df.sort_values('MAE_mean', ascending=True).reset_index(drop=True)\nresults_by_mae.insert(0, 'Rank', range(1, len(results_by_mae) + 1))\nmae_filename = f'all_params_ranked_by_MAE_{timestamp}.csv'\nresults_by_mae.to_csv(mae_filename, index=False)\nprint(f\"✓ MAE rankings saved: {mae_filename}\")\n\n# 4. Sorted by MAPE\nresults_by_mape = results_df.sort_values('MAPE_mean', ascending=True).reset_index(drop=True)\nresults_by_mape.insert(0, 'Rank', range(1, len(results_by_mape) + 1))\nmape_filename = f'all_params_ranked_by_MAPE_{timestamp}.csv'\nresults_by_mape.to_csv(mape_filename, index=False)\nprint(f\"✓ MAPE rankings saved: {mape_filename}\")\n\n# === Calculate SPP Range Performance for Top 10 Models ===\nprint(\"\\n\" + \"=\"*50)\nprint(\"CALCULATING SPP RANGE PERFORMANCE FOR TOP 10 MODELS\")\nprint(\"=\"*50)\n\ndef calculate_top10_spp_metrics(ranked_df, metric_name):\n    \"\"\"Calculate SPP range metrics for top 10 models\"\"\"\n    top10_spp_metrics = []\n    \n    for idx, row in tqdm(ranked_df.head(10).iterrows(), total=10, desc=f\"Top 10 by {metric_name}\"):\n        params = eval(row['params'])\n        \n        # Create model with these parameters\n        model = CatBoostRegressor(\n            **params,\n            random_seed=42,\n            cat_features=categorical_features,\n            verbose=False,\n            task_type='GPU'\n        )\n        \n        # Get CV predictions\n        y_pred_log = cross_val_predict(model, X, y, cv=cv, n_jobs=1)\n        y_pred_spp = np.expm1(y_pred_log)\n        y_true_spp = y_spp_original.values\n        \n        # Calculate range metrics\n        range_metrics = calculate_spp_range_metrics(y_true_spp, y_pred_spp)\n        \n        # Combine with existing metrics\n        combined_metrics = {\n            'Rank': row['Rank'],\n            'params': row['params'],\n            'MAE_mean': row['MAE_mean'],\n            'MAE_std': row['MAE_std'],\n            'MAPE_mean': row['MAPE_mean'],\n            'MAPE_std': row['MAPE_std'],\n            'R2_log_mean': row['R2_log_mean'],\n            'R2_log_std': row['R2_log_std'],\n            'R2_original_mean': row['R2_original_mean'],\n            'R2_original_std': row['R2_original_std'],\n            **range_metrics\n        }\n        \n        top10_spp_metrics.append(combined_metrics)\n    \n    return pd.DataFrame(top10_spp_metrics)\n\n# Calculate for each ranking\ntop10_r2_orig_spp = calculate_top10_spp_metrics(results_by_r2_orig, \"R² Original\")\ntop10_r2_log_spp = calculate_top10_spp_metrics(results_by_r2_log, \"R² Log\")\ntop10_mae_spp = calculate_top10_spp_metrics(results_by_mae, \"MAE\")\ntop10_mape_spp = calculate_top10_spp_metrics(results_by_mape, \"MAPE\")\n\n# Save top 10 with SPP range metrics\ntop10_r2_orig_spp.to_csv(f'top10_by_R2_original_with_spp_ranges_{timestamp}.csv', index=False)\ntop10_r2_log_spp.to_csv(f'top10_by_R2_log_with_spp_ranges_{timestamp}.csv', index=False)\ntop10_mae_spp.to_csv(f'top10_by_MAE_with_spp_ranges_{timestamp}.csv', index=False)\ntop10_mape_spp.to_csv(f'top10_by_MAPE_with_spp_ranges_{timestamp}.csv', index=False)\n\nprint(f\"\\n✓ Top 10 models with SPP range analysis saved\")\n\n# === Train and Save Top 10 Models for Each Metric ===\nprint(\"\\n\" + \"=\"*50)\nprint(\"TRAINING AND SAVING TOP 10 MODELS FOR EACH METRIC\")\nprint(\"=\"*50)\n\nos.makedirs(f'top_models_by_R2_original_{timestamp}', exist_ok=True)\nos.makedirs(f'top_models_by_R2_log_{timestamp}', exist_ok=True)\nos.makedirs(f'top_models_by_MAE_{timestamp}', exist_ok=True)\nos.makedirs(f'top_models_by_MAPE_{timestamp}', exist_ok=True)\n\n# Function to train and save models\ndef train_and_save_top10(ranked_df_with_spp, folder_name, metric_suffix):\n    models_info = []\n    \n    for idx, row in tqdm(ranked_df_with_spp.iterrows(), total=len(ranked_df_with_spp), desc=f\"Training {metric_suffix}\"):\n        params = eval(row['params'])\n        \n        model = CatBoostRegressor(\n            **params,\n            random_seed=42,\n            cat_features=categorical_features,\n            verbose=False,\n            task_type='GPU'\n        )\n        model.fit(X, y)\n        \n        model_info = row.to_dict()\n        \n        # Create filename\n        if 'R2' in metric_suffix:\n            metric_val = row['R2_original_mean'] if 'original' in metric_suffix else row['R2_log_mean']\n            metric_std = row['R2_original_std'] if 'original' in metric_suffix else row['R2_log_std']\n            filename = f\"rank_{int(row['Rank'])}_{metric_suffix}_{metric_val:.4f}_std_{metric_std:.4f}.pkl\"\n        else:\n            metric_val = row['MAE_mean'] if metric_suffix == 'MAE' else row['MAPE_mean']\n            metric_std = row['MAE_std'] if metric_suffix == 'MAE' else row['MAPE_std']\n            filename = f\"rank_{int(row['Rank'])}_{metric_suffix}_{metric_val:.2f}_std_{metric_std:.2f}.pkl\"\n        \n        model_path = os.path.join(folder_name, filename)\n        joblib.dump({'model': model, 'info': model_info}, model_path)\n        models_info.append(model_info)\n    \n    # Save summary\n    with open(os.path.join(folder_name, 'summary.json'), 'w') as f:\n        json.dump(models_info, f, indent=4)\n    \n    return models_info\n\n# Train all top 10 models\nprint(\"\\nTraining Top 10 by R² (original)...\")\ntrain_and_save_top10(top10_r2_orig_spp, f'top_models_by_R2_original_{timestamp}', 'R2_original')\n\nprint(\"\\nTraining Top 10 by R² (log)...\")\ntrain_and_save_top10(top10_r2_log_spp, f'top_models_by_R2_log_{timestamp}', 'R2_log')\n\nprint(\"\\nTraining Top 10 by MAE...\")\ntrain_and_save_top10(top10_mae_spp, f'top_models_by_MAE_{timestamp}', 'MAE')\n\nprint(\"\\nTraining Top 10 by MAPE...\")\ntrain_and_save_top10(top10_mape_spp, f'top_models_by_MAPE_{timestamp}', 'MAPE')\n\n# === Display Top 10 with SPP Range Metrics ===\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 10 MODELS BY R² (ORIGINAL) - WITH SPP RANGE PERFORMANCE\")\nprint(\"=\"*80)\ndisplay_cols = ['Rank', 'R2_original_mean', 'R2_original_std', 'MAE_mean', \n                'SPP_<1M_MAE', 'SPP_1M-3M_MAE', 'SPP_>=3M_MAE']\nprint(top10_r2_orig_spp[display_cols].to_string(index=False))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 10 MODELS BY R² (LOG) - WITH SPP RANGE PERFORMANCE\")\nprint(\"=\"*80)\ndisplay_cols = ['Rank', 'R2_log_mean', 'R2_log_std', 'MAE_mean',\n                'SPP_<1M_MAE', 'SPP_1M-3M_MAE', 'SPP_>=3M_MAE']\nprint(top10_r2_log_spp[display_cols].to_string(index=False))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 10 MODELS BY MAE - WITH SPP RANGE PERFORMANCE\")\nprint(\"=\"*80)\ndisplay_cols = ['Rank', 'MAE_mean', 'MAE_std', 'R2_original_mean',\n                'SPP_<1M_MAE', 'SPP_1M-3M_MAE', 'SPP_>=3M_MAE']\nprint(top10_mae_spp[display_cols].to_string(index=False))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 10 MODELS BY MAPE - WITH SPP RANGE PERFORMANCE\")\nprint(\"=\"*80)\ndisplay_cols = ['Rank', 'MAPE_mean', 'MAPE_std', 'R2_original_mean',\n                'SPP_<1M_MAPE', 'SPP_1M-3M_MAPE', 'SPP_>=3M_MAPE']\nprint(top10_mape_spp[display_cols].to_string(index=False))\n\n# === Train Final Best Model ===\nbest_params = eval(top10_r2_orig_spp.iloc[0]['params'])\nprint(\"\\n\" + \"=\"*50)\nprint(\"TRAINING FINAL BEST MODEL (BY R² ORIGINAL)\")\nprint(\"=\"*50)\n\nfinal_model = CatBoostRegressor(\n    **best_params,\n    random_seed=42,\n    cat_features=categorical_features,\n    verbose=100,\n    task_type='GPU'\n)\n\nfinal_model.fit(X, y)\n\n# Get predictions for visualization\ny_pred_log_best = cross_val_predict(final_model, X, y, cv=cv, n_jobs=1)\ny_actual_spp = np.expm1(y.values)\ny_predicted_spp = np.expm1(y_pred_log_best)\n\n# === Feature Importance ===\nfeature_importance = final_model.get_feature_importance()\nimportance_df = pd.DataFrame({\n    'feature': features,\n    'importance': feature_importance\n}).sort_values('importance', ascending=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"TOP 10 FEATURE IMPORTANCE\")\nprint(\"=\"*50)\nprint(importance_df.head(10).to_string(index=False))\n\n# === Visualization ===\nfig, axes = plt.subplots(1, 2, figsize=(18, 7))\n\n# Plot 1: Predictions\npredictions_df = pd.DataFrame({\n    'Real_SPP': y_actual_spp,\n    'Predicted_SPP': y_predicted_spp\n})\n\naxes[0].scatter(predictions_df['Real_SPP'], predictions_df['Predicted_SPP'], alpha=0.6)\naxes[0].set_xlabel('Real SPP')\naxes[0].set_ylabel('Predicted SPP')\naxes[0].set_title('Real vs Predicted SPP - Best Model (Log-Log Scale)')\naxes[0].set_xscale('log')\naxes[0].set_yscale('log')\naxes[0].grid(True, alpha=0.3)\n\nmin_spp = predictions_df[['Real_SPP', 'Predicted_SPP']].min().min()\nmax_spp = predictions_df[['Real_SPP', 'Predicted_SPP']].max().max()\naxes[0].plot([min_spp, max_spp], [min_spp, max_spp], 'r--', label='Perfect Prediction', linewidth=2)\naxes[0].legend()\n\n# Plot 2: Feature Importance\ntop_features = importance_df.head(15)\naxes[1].barh(range(len(top_features)), top_features['importance'])\naxes[1].set_yticks(range(len(top_features)))\naxes[1].set_yticklabels(top_features['feature'])\naxes[1].set_xlabel('Feature Importance')\naxes[1].set_title('Top 15 Feature Importance')\naxes[1].invert_yaxis()\n\nplt.tight_layout()\nplt.savefig(f'model_performance_{timestamp}.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# === Save Additional Files ===\nmodel_filename = f'best_model_by_R2_{timestamp}.pkl'\njoblib.dump(final_model, model_filename)\nprint(f\"\\n✓ Best model saved: {model_filename}\")\n\nfeature_filename = f'model_features_{timestamp}.pkl'\njoblib.dump({\n    'features': features,\n    'categorical_features': categorical_features\n}, feature_filename)\nprint(f\"✓ Features saved: {feature_filename}\")\n\nimportance_filename = f'feature_importance_{timestamp}.csv'\nimportance_df.to_csv(importance_filename, index=False)\nprint(f\"✓ Feature importance saved: {importance_filename}\")\n\n# === Final Summary ===\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING COMPLETE - COMPREHENSIVE SUMMARY\")\nprint(\"=\"*80)\nprint(f\"\\nTotal parameter combinations tested: {len(results_df)}\")\nprint(f\"Total models trained and saved: 40 (10 per metric)\")\n\nprint(f\"\\n📊 FILES CREATED:\")\nprint(f\"\\n1. RANKING FILES (All {len(results_df)} combinations):\")\nprint(f\"   ✓ {r2_orig_filename}\")\nprint(f\"   ✓ {r2_log_filename}\")\nprint(f\"   ✓ {mae_filename}\")\nprint(f\"   ✓ {mape_filename}\")\n\nprint(f\"\\n2. TOP 10 WITH SPP RANGE ANALYSIS:\")\nprint(f\"   ✓ top10_by_R2_original_with_spp_ranges_{timestamp}.csv\")\nprint(f\"   ✓ top10_by_R2_log_with_spp_ranges_{timestamp}.csv\")\nprint(f\"   ✓ top10_by_MAE_with_spp_ranges_{timestamp}.csv\")\nprint(f\"   ✓ top10_by_MAPE_with_spp_ranges_{timestamp}.csv\")\n\nprint(f\"\\n3. MODEL FOLDERS (40 trained models):\")\nprint(f\"   ✓ top_models_by_R2_original_{timestamp}/ (10 models + summary.json)\")\nprint(f\"   ✓ top_models_by_R2_log_{timestamp}/ (10 models + summary.json)\")\nprint(f\"   ✓ top_models_by_MAE_{timestamp}/ (10 models + summary.json)\")\nprint(f\"   ✓ top_models_by_MAPE_{timestamp}/ (10 models + summary.json)\")\n\nprint(f\"\\n4. BEST MODEL & SUPPORTING FILES:\")\nprint(f\"   ✓ {model_filename}\")\nprint(f\"   ✓ {feature_filename}\")\nprint(f\"   ✓ {importance_filename}\")\nprint(f\"   ✓ model_performance_{timestamp}.png\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SPP RANGE METRICS INCLUDED FOR TOP 10:\")\nprint(\"=\"*80)\nprint(\"For each top 10 model, the following SPP range metrics are calculated:\")\nprint(\"  • SPP < 1M: MAE, MAPE, count\")\nprint(\"  • SPP 1M-3M: MAE, MAPE, count\")\nprint(\"  • SPP >= 3M: MAE, MAPE, count\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"HOW TO USE THE RESULTS\")\nprint(\"=\"*80)\nprint(\"\"\"\n1. ANALYZE SPP RANGE PERFORMANCE:\n   - Open top10_by_[METRIC]_with_spp_ranges_[timestamp].csv files\n   - Compare how models perform across different SPP ranges\n   - Choose models that perform well in your target SPP range\n\n2. LOAD A TRAINED MODEL:\n   import joblib\n   import numpy as np\n   \n   # Load a specific model\n   model_data = joblib.load('top_models_by_R2_original_[timestamp]/rank_1_[...].pkl')\n   model = model_data['model']\n   info = model_data['info']\n   \n   # Check SPP range performance\n   print(f\"SPP <1M MAE: {info['SPP_<1M_MAE']:.2f}\")\n   print(f\"SPP 1M-3M MAE: {info['SPP_1M-3M_MAE']:.2f}\")\n   print(f\"SPP >=3M MAE: {info['SPP_>=3M_MAE']:.2f}\")\n   \n   # Make predictions\n   predictions_log = model.predict(X_new)\n   predictions_spp = np.expm1(predictions_log)\n\n3. CHOOSE THE BEST MODEL FOR YOUR USE CASE:\n   - If you care about all SPP ranges equally → Use top model by R² or MAE\n   - If you care about low SPP schools → Check SPP_<1M_MAE in the CSV\n   - If you care about high SPP schools → Check SPP_>=3M_MAE in the CSV\n   - If you want stability → Choose models with low std values\n\n4. VIEW DETAILED METRICS:\n   - All ranking CSVs show mean and std for all metrics\n   - Top 10 CSVs include SPP range breakdown\n   - summary.json files contain complete model information\n\"\"\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXAMPLE: FINDING BEST MODEL FOR LOW SPP SCHOOLS (<1M)\")\nprint(\"=\"*80)\nprint(\"\"\"\nimport pandas as pd\n\n# Load top 10 by MAE with SPP ranges\ndf = pd.read_csv('top10_by_MAE_with_spp_ranges_[timestamp].csv')\n\n# Sort by SPP_<1M_MAE to find best for low SPP schools\ndf_sorted = df.sort_values('SPP_<1M_MAE')\nprint(df_sorted[['Rank', 'SPP_<1M_MAE', 'SPP_<1M_MAPE', 'MAE_mean']].head())\n\n# Load that specific model\nbest_for_low_spp = df_sorted.iloc[0]\n# Use the rank to find the model file in the appropriate folder\n\"\"\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"KEY INSIGHTS FROM SPP RANGE ANALYSIS\")\nprint(\"=\"*80)\n\n# Calculate and display insights\nfor metric_name, df_spp in [\n    (\"R² Original\", top10_r2_orig_spp),\n    (\"R² Log\", top10_r2_log_spp),\n    (\"MAE\", top10_mae_spp),\n    (\"MAPE\", top10_mape_spp)\n]:\n    print(f\"\\nTop model by {metric_name}:\")\n    top_model = df_spp.iloc[0]\n    print(f\"  Overall MAE: {top_model['MAE_mean']:.2f}\")\n    print(f\"  SPP <1M MAE: {top_model['SPP_<1M_MAE']:.2f} ({top_model['SPP_<1M_count']} schools)\")\n    print(f\"  SPP 1M-3M MAE: {top_model['SPP_1M-3M_MAE']:.2f} ({top_model['SPP_1M-3M_count']} schools)\")\n    print(f\"  SPP >=3M MAE: {top_model['SPP_>=3M_MAE']:.2f} ({top_model['SPP_>=3M_count']} schools)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING COMPLETE ✓\")\nprint(\"=\"*80)\nprint(f\"\\nAll files saved with timestamp: {timestamp}\")\nprint(\"Check the output files to select the best model for your specific use case!\")\nprint(\"\\n\" + \"=\"*80)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:48:47.639447Z","iopub.execute_input":"2025-09-26T16:48:47.639629Z","execution_failed":"2025-09-26T16:52:21.270Z"}},"outputs":[],"execution_count":null}]}